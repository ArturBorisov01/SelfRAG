{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1oWfXlBS75GtJtjyeszOtI9CrD6Rnd2uK","authorship_tag":"ABX9TyOK+nUdxX6tZgPWnALrbfaK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip download PyPDF2 -d /content/drive/MyDrive/Rag_Model_files/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ng9tJJLoHNb-","executionInfo":{"status":"ok","timestamp":1739029879553,"user_tz":-300,"elapsed":1922,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"670cdcb1-f5da-4905-d3f4-211580bef39c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hSaved ./drive/MyDrive/Rag_Model_files/pypdf2-3.0.1-py3-none-any.whl\n","Successfully downloaded PyPDF2\n"]}]},{"cell_type":"code","source":["!pip download openai -d /content/drive/MyDrive/Rag_Model_files/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqnM6mjkaNI8","executionInfo":{"status":"ok","timestamp":1739032452931,"user_tz":-300,"elapsed":9487,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"dd2f5539-05c8-4a01-a9a9-c8ddaa95725e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.61.1-py3-none-any.whl.metadata (27 kB)\n","Collecting anyio<5,>=3.5.0 (from openai)\n","  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting distro<2,>=1.7.0 (from openai)\n","  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting jiter<1,>=0.4.0 (from openai)\n","  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n","Collecting pydantic<3,>=1.9.0 (from openai)\n","  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n","Collecting sniffio (from openai)\n","  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting tqdm>4 (from openai)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions<5,>=4.11 (from openai)\n","  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai)\n","  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n","Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n","  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n","Collecting certifi (from httpx<1,>=0.23.0->openai)\n","  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n","  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n","Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n","  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Downloading openai-1.61.1-py3-none-any.whl (463 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n","Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n","Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n","Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n","Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n","Downloading idna-3.10-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hSaved ./drive/MyDrive/Rag_Model_files/openai-1.61.1-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/anyio-4.8.0-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/distro-1.9.0-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/httpx-0.28.1-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/httpcore-1.0.7-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./drive/MyDrive/Rag_Model_files/pydantic-2.10.6-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./drive/MyDrive/Rag_Model_files/sniffio-1.3.1-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/tqdm-4.67.1-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/typing_extensions-4.12.2-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/annotated_types-0.7.0-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/exceptiongroup-1.2.2-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/idna-3.10-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/certifi-2025.1.31-py3-none-any.whl\n","Saved ./drive/MyDrive/Rag_Model_files/h11-0.14.0-py3-none-any.whl\n","Successfully downloaded openai anyio distro httpx httpcore jiter pydantic pydantic-core sniffio tqdm typing-extensions annotated-types exceptiongroup idna certifi h11\n"]}]},{"cell_type":"code","source":["!pip install /content/drive/MyDrive/Rag_Model_files/pypdf2-3.0.1-py3-none-any.whl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G97O3kQlJWIm","executionInfo":{"status":"ok","timestamp":1739095597641,"user_tz":-300,"elapsed":2291,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"8b7af64b-a093-4bf6-c9f0-7d8545a7d667"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing ./drive/MyDrive/Rag_Model_files/pypdf2-3.0.1-py3-none-any.whl\n","pypdf2 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"]}]},{"cell_type":"code","source":["!pip install /content/drive/MyDrive/Rag_Model_files/openAI/*.whl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnP__UaDabS_","executionInfo":{"status":"ok","timestamp":1739095601031,"user_tz":-300,"elapsed":1899,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"71559fcd-0cf4-4e77-87c8-ab0ed1966a28"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing ./drive/MyDrive/Rag_Model_files/openAI/annotated_types-0.7.0-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/anyio-4.8.0-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/certifi-2025.1.31-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/distro-1.9.0-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/exceptiongroup-1.2.2-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/h11-0.14.0-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/httpcore-1.0.7-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/httpx-0.28.1-py3-none-any.whl\n","Processing ./drive/MyDrive/Rag_Model_files/openAI/idna-3.10-py3-none-any.whl\n","\u001b[31mERROR: jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":131,"metadata":{"id":"cSZGJQuxD4SB","executionInfo":{"status":"ok","timestamp":1739095603052,"user_tz":-300,"elapsed":67,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"outputs":[],"source":["# Prepare chunks from PDF or CSV\n","\n","import csv\n","import PyPDF2\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","def process_pdf(pdf_path):\n","    with open(pdf_path, 'rb') as pdf_file:\n","        pdf_reader = PyPDF2.PdfReader(pdf_file)\n","        text = \"\"\n","        for page_num in range(len(pdf_reader.pages)):\n","            page = pdf_reader.pages[page_num]\n","            text += page.extract_text()\n","    return text\n","\n","def process_csv(csv_path):\n","    with open(csv_path, 'r', encoding='utf-8') as csv_file:\n","        reader = csv.reader(csv_file)\n","        text = \"\"\n","        for row in reader:\n","            text += ' '.join(row)\n","    return text\n","\n","\n","# from langchain.text_splitter import RecursiveCharacterTextSplitter\n","text_splitter = RecursiveCharacterTextSplitter(\n","      chunk_size=1000,\n","      chunk_overlap=200,\n","      separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n","  )\n","\n",""]},{"cell_type":"code","source":["# take needed function for opening file in true form\n","def process_file(file_path):\n","\n","    file_type = file_path.split('.')[-1].lower()\n","    if file_type == 'pdf':\n","        text = process_pdf(file_path)\n","    elif file_type == 'csv':\n","        text = process_csv(file_path)\n","    else:\n","        raise ValueError(\"Unsupported file type. Please provide a PDF or CSV file.\")\n","\n","    return text_splitter.split_text(text)"],"metadata":{"id":"tDnWs2VMKuZA","executionInfo":{"status":"ok","timestamp":1739095608364,"user_tz":-300,"elapsed":42,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","\n","from openai import OpenAI\n","\n","# os.environ[\"OPENAI_API_KEY\"] = \"Key\"\n","api_key = os.environ.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=api_key)\n","\n","text_embeddings_dict = {}\n","\n","file_path = '/content/drive/MyDrive/Rag_Model_files/doc1.pdf'\n","\n","chunks = process_file(file_path)\n","\n","for i, chunk in enumerate(chunks):\n","  response = client.embeddings.create(\n","      input=chunk,\n","      model=\"text-embedding-3-small\"\n","  )\n","\n","  embedding = response.data[0].embedding\n","\n","  text_embeddings_dict[chunk] = {\n","        \"text\": chunk,\n","        \"embedding\": embedding\n","    }\n","\n","# print(response.data[0].embedding)\n","\n","name_file_embedings = '/content/drive/MyDrive/Rag_Model_files/text_embeddings_doc1_symb.json'\n","\n","with open(name_file_embedings, 'w') as f:\n","    json.dump(text_embeddings_dict, f)"],"metadata":{"id":"9kCFTSIGUp1R","executionInfo":{"status":"ok","timestamp":1739093338814,"user_tz":-300,"elapsed":107797,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["def to_embedding_question(question):\n","    response = client.embeddings.create(\n","      input=question,\n","      model=\"text-embedding-3-small\"\n","    )\n","    return response.data[0].embedding\n","\n","\n","question = \"Which benchmarks was compared in subsection 5.3 <compared baselines>?\"\n","\n","question_embedding = to_embedding_question(question)\n","embedding_data = {\n","    \"question\": question,\n","    \"embedding\": question_embedding\n","}\n","\n","with open('/content/drive/MyDrive/Rag_Model_files/question_Which benchmarks_embedding.json', 'w') as f:\n","    json.dump(embedding_data, f)"],"metadata":{"id":"Gw5tC56jrfen","executionInfo":{"status":"ok","timestamp":1739092641598,"user_tz":-300,"elapsed":256,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import NearestNeighbors\n","import numpy as np\n","\n","def find_best_chunks(question_embedding, chunk_embeddings, num_neighbors):\n","\n","    question_embedding_2d = np.array(question_embedding).reshape(1, -1)\n","    chunk_embeddings_np = np.array(chunk_embeddings)\n","\n","\n","    knn = NearestNeighbors(n_neighbors=num_neighbors, metric='cosine')\n","    knn.fit(chunk_embeddings_np)\n","    distances, indices = knn.kneighbors(question_embedding_2d)\n","\n","    best_chunk_indices = indices[0]\n","    return best_chunk_indices"],"metadata":{"id":"9ZndOHPs2Fw9","executionInfo":{"status":"ok","timestamp":1739095631461,"user_tz":-300,"elapsed":4,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"execution_count":133,"outputs":[]},{"cell_type":"code","source":["import json\n","import numpy as np\n","from sklearn.neighbors import NearestNeighbors\n","\n","\n","# Load chunk embeddings\n","with open('/content/drive/MyDrive/Rag_Model_files/text_embeddings_DeepSeek_symb.json', 'r') as f:\n","    chunk_data = json.load(f)\n","chunk_embeddings = []\n","for chunk, data in chunk_data.items():\n","    chunk_embeddings.append(data['embedding'])\n","\n","\n","# Load question embedding\n","with open('/content/drive/MyDrive/Rag_Model_files/question_Which benchmarks_embedding.json', 'r') as f:\n","    question_data = json.load(f)\n","question_embedding = question_data['embedding']\n","\n","\n","# Find best chunks iindices\n","num_neighbors = 5\n","\n","\n","best_chunk_indices = find_best_chunks(question_embedding, chunk_embeddings, num_neighbors)\n","print(f\"Indices of the best chunks: {best_chunk_indices}\")\n","\n","\n","# Access the best chunks using the indices\n","best_chunks = []\n","for i, (chunk, data) in enumerate(chunk_data.items()):\n","  if i in best_chunk_indices:\n","    best_chunks.append(data['text'])\n","    print(f\"Chunk {i}: {data['text']}\")\n","\n","# print(f\"\\nBest chunks:\\n{best_chunks}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EuxysS2x6RRR","executionInfo":{"status":"ok","timestamp":1739092673409,"user_tz":-300,"elapsed":209,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"a7d4a666-6fa5-430d-ee55-4dddd2f1136b"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Indices of the best chunks: [122 114 127  94 115]\n","Chunk 94: with our internal evaluation framework, and ensure that they share the same evaluation setting.\n","Note that due to the changes in our evaluation framework over the past months, the performance\n","of DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall,\n","DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base,\n","and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the\n","strongest open-source model.\n","25From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source\n","base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in\n","our model architecture, the scale-up of the model size and training tokens, and the enhancement\n","of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2)\n","Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only\n","Chunk 114: models, evaluations are performed through their respective APIs.\n","1https://aider.chat\n","2https://codeforces.com\n","3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n","30Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP ,\n","GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4.\n","We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting.\n","For other datasets, we follow their original evaluation protocols with default prompts as pro-\n","vided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset\n","includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript,\n","PHP , and Bash) in total. We use CoT and non-CoT methods to evaluate model performance\n","on LiveCodeBench, where the data are collected from August 2024 to November 2024. The\n","Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is\n","Chunk 115: on LiveCodeBench, where the data are collected from August 2024 to November 2024. The\n","Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is\n","evaluated using the agentless framework (Xia et al., 2024). We use the “diff” format to evaluate\n","the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are\n","evaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500\n","employs greedy decoding. We allow all models to output a maximum of 8192 tokens for each\n","benchmark.\n","Benchmark (Metric)DeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o DeepSeek\n","V2-0506 V2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022 0513 V3\n","Architecture MoE MoE Dense Dense - - MoE\n","# Activated Params 21B 21B 72B 405B - - 37B\n","# Total Params 236B 236B 72B 405B - - 671B\n","EnglishMMLU (EM) 78.2 80.6 85.3 88.6 88.3 87.2 88.5\n","MMLU-Redux (EM) 77.9 80.3 85.6 86.2 88.9 88.0 89.1\n","MMLU-Pro (EM) 58.5 66.2 71.6 73.3 78.0 72.6 75.9\n","Chunk 122: all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\n","attributed to its advanced knowledge distillation technique, which effectively enhances its code\n","generation and problem-solving capabilities in algorithm-focused tasks.\n","On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\n","surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\n","AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\n","72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\n","benchmarks. This remarkable capability highlights the effectiveness of the distillation technique\n","from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\n","32Model Arena-Hard AlpacaEval 2.0\n","DeepSeek-V2.5-0905 76.2 50.5\n","Qwen2.5-72B-Instruct 81.2 49.1\n","LLaMA-3.1 405B 69.3 40.5\n","GPT-4o-0513 80.4 51.1\n","Claude-Sonnet-3.5-1022 85.2 52.0\n","DeepSeek-V3 85.5 70.0\n","Chunk 127: GPT-4o-1120 95.8 71.3 86.2 85.2 84.6\n","Claude-3.5-sonnet-0620 96.4 74.0 81.6 84.7 84.2\n","Claude-3.5-sonnet-1022 96.4 79.7 91.1 87.6 88.7\n","DeepSeek-V3 96.9 79.8 87.0 84.3 87.0\n","DeepSeek-V3 (maj@6) 96.9 82.6 89.5 89.2 89.6\n","Table 8|Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench.\n","ModelLiveCodeBench-CoT MATH-500\n","Pass@1 Length Pass@1 Length\n","DeepSeek-V2.5 Baseline 31.1 718 74.6 769\n","DeepSeek-V2.5 +R1 Distill 37.4 783 83.2 1510\n","Table 9|The contribution of distillation from DeepSeek-R1. The evaluation settings of Live-\n","CodeBench and MATH-500 are the same as in Table 6.\n","effectiveness and robustness of the alignment process.\n","5.4. Discussion\n","5.4.1. Distillation from DeepSeek-R1\n","We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The\n","baseline is trained on short CoT data, whereas its competitor uses data generated by the expert\n","checkpoints described above.\n","\n","Best chunks:\n","['with our internal evaluation framework, and ensure that they share the same evaluation setting.\\nNote that due to the changes in our evaluation framework over the past months, the performance\\nof DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall,\\nDeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base,\\nand surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the\\nstrongest open-source model.\\n25From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source\\nbase models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in\\nour model architecture, the scale-up of the model size and training tokens, and the enhancement\\nof data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2)\\nCompared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only', 'models, evaluations are performed through their respective APIs.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n30Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP ,\\nGPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4.\\nWe utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting.\\nFor other datasets, we follow their original evaluation protocols with default prompts as pro-\\nvided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset\\nincludes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript,\\nPHP , and Bash) in total. We use CoT and non-CoT methods to evaluate model performance\\non LiveCodeBench, where the data are collected from August 2024 to November 2024. The\\nCodeforces dataset is measured using the percentage of competitors. SWE-Bench verified is', 'on LiveCodeBench, where the data are collected from August 2024 to November 2024. The\\nCodeforces dataset is measured using the percentage of competitors. SWE-Bench verified is\\nevaluated using the agentless framework (Xia et al., 2024). We use the “diff” format to evaluate\\nthe Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are\\nevaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500\\nemploys greedy decoding. We allow all models to output a maximum of 8192 tokens for each\\nbenchmark.\\nBenchmark (Metric)DeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o DeepSeek\\nV2-0506 V2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022 0513 V3\\nArchitecture MoE MoE Dense Dense - - MoE\\n# Activated Params 21B 21B 72B 405B - - 37B\\n# Total Params 236B 236B 72B 405B - - 671B\\nEnglishMMLU (EM) 78.2 80.6 85.3 88.6 88.3 87.2 88.5\\nMMLU-Redux (EM) 77.9 80.3 85.6 86.2 88.9 88.0 89.1\\nMMLU-Pro (EM) 58.5 66.2 71.6 73.3 78.0 72.6 75.9', 'all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\\nattributed to its advanced knowledge distillation technique, which effectively enhances its code\\ngeneration and problem-solving capabilities in algorithm-focused tasks.\\nOn math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\\nsurpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\\nAIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\\n72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\\nbenchmarks. This remarkable capability highlights the effectiveness of the distillation technique\\nfrom DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\\n32Model Arena-Hard AlpacaEval 2.0\\nDeepSeek-V2.5-0905 76.2 50.5\\nQwen2.5-72B-Instruct 81.2 49.1\\nLLaMA-3.1 405B 69.3 40.5\\nGPT-4o-0513 80.4 51.1\\nClaude-Sonnet-3.5-1022 85.2 52.0\\nDeepSeek-V3 85.5 70.0', 'GPT-4o-1120 95.8 71.3 86.2 85.2 84.6\\nClaude-3.5-sonnet-0620 96.4 74.0 81.6 84.7 84.2\\nClaude-3.5-sonnet-1022 96.4 79.7 91.1 87.6 88.7\\nDeepSeek-V3 96.9 79.8 87.0 84.3 87.0\\nDeepSeek-V3 (maj@6) 96.9 82.6 89.5 89.2 89.6\\nTable 8|Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench.\\nModelLiveCodeBench-CoT MATH-500\\nPass@1 Length Pass@1 Length\\nDeepSeek-V2.5 Baseline 31.1 718 74.6 769\\nDeepSeek-V2.5 +R1 Distill 37.4 783 83.2 1510\\nTable 9|The contribution of distillation from DeepSeek-R1. The evaluation settings of Live-\\nCodeBench and MATH-500 are the same as in Table 6.\\neffectiveness and robustness of the alignment process.\\n5.4. Discussion\\n5.4.1. Distillation from DeepSeek-R1\\nWe ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The\\nbaseline is trained on short CoT data, whereas its competitor uses data generated by the expert\\ncheckpoints described above.']\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","\n","from openai import OpenAI\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"Key\"\n","api_key = os.environ.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=api_key)\n","\n","\n","def to_embedding_question(question):\n","    response = client.embeddings.create(\n","      input=question,\n","      model=\"text-embedding-3-small\"\n","    )\n","    return response.data[0].embedding\n","\n","\n","def answer_question(question, chunk_data, num_neighbors=5):\n","\n","    #prepare question\n","    question_embedding = to_embedding_question(question)\n","\n","    #prepare embeding for find best chunk\n","    chunk_embeddings = []\n","    for chunk, data in chunk_data.items():\n","        chunk_embeddings.append(data['embedding'])\n","\n","\n","    best_chunk_indices = find_best_chunks(question_embedding, chunk_embeddings, num_neighbors)\n","    best_chunks = []\n","    for i, (chunk, data) in enumerate(chunk_data.items()):\n","        if i in best_chunk_indices:\n","            best_chunks.append(data['text'])\n","\n","\n","\n","    prompt = f\"\"\"Отвечай на вопрос используя только Контекст. Если конекст не содержит ответ, скажи Я не знаю.\n","\n","Вопрос: {question}\n","\n","Контекст: {''.join(best_chunks)}\n","\n","\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    )\n","\n","    answer = response.choices[0].message.content.strip()\n","    return answer"],"metadata":{"id":"xeDV_A2dFkj0","executionInfo":{"status":"ok","timestamp":1739095848510,"user_tz":-300,"elapsed":268,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}}},"execution_count":140,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","# name_file_embedings = '/content/drive/MyDrive/Rag_Model_files/text_embeddings_doc1_symb.json'\n","\n","name_file_embedings = '/content/drive/MyDrive/Rag_Model_files/text_embeddings_DeepSeek_symb.json'\n","\n","with open(name_file_embedings, 'r') as f:\n","    chunk_data = json.load(f)\n","\n","question = \"Какой сегодня день недели?\"\n","# question = \"Какой сайт банка?\"\n","\n","answer = answer_question(question, chunk_data)\n","print(f\"Вопрос: {question}\\nОтвет: {answer}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KfHjXfzNMCZ","executionInfo":{"status":"ok","timestamp":1739096450419,"user_tz":-300,"elapsed":694,"user":{"displayName":"Artur Borisov","userId":"12151746721980349383"}},"outputId":"deadbfac-9980-45c0-e8f3-0b1d82ec816a"},"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["Вопрос: Какой сегодня день недели?\n","Ответ: Я не знаю.\n"]}]}]}